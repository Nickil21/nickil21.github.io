<p>I am thrilled to be a part of the awesome Red Hen Lab community! Thank you for selecting me and giving me a chance to contribute to the Red Hen codebase.</p>
<p>This post describes my journey after being selected as a Google Summer of Code (GSoC) student associated with Red Hen Lab and FrameNet Brasil. I plan to summarize my progress at the end of every week until the end of the summer.</p>
<p>Here’s the abstract of my project:</p>
<blockquote>
<p>The project aims to develop a prototype that is capable of meaning construction using multi-modal channels of communication. Specifically, for a co-speech gestures dataset, using the annotations obtained manually coupled with the metadata obtained through algorithms, we devise a mechanism to disambiguate meaning considering the influence of all the different modalities involved in a particular frame. Since only a handful of annotated datasets have been made available by Red Hen, we leverage semi-supervised learning techniques to annotate additional unlabeled data. Furthermore, since each frame could have multiple meaning interpretations possible, we use human annotators to annotate a subset of our validation set and report our performance on that set.</p>
</blockquote>
<p>My mentors are: <a href="https://github.com/suziexi">Suzie Xi</a>{:target="_blank“}, <a href="https:markturner.org">Mark Turner</a>{:target=”_blank“}, <a href="https://www.um.es/lincoing/jv/index_en.htm">Javier Valenzuela</a>{:target=”_blank“}, <a href="https://www.rees.ox.ac.uk/people/dr-anna-wilson">Anna Wilson</a>{:target=”_blank“}, <a href="https://www.mariamhedblom.com">Maria Hedblom</a>{:target=”_blank“}, <a href="https://comm.ucla.edu/person/francis-steen">Francis Steen</a>{:target=”_blank“}, <a href="https://www.tiagotorrent.com">Tiago Torrent</a>{:target=”_blank“} (primary), <a href="https://www.researchgate.net/profile/Frederico-Belcavello">Frederico Belcavello</a>{:target=”_blank“}, <a href="https://sites.google.com/site/inesolza/">Inés Olza</a>{:target=”_blank"}.</p>
<h2 id="stay-tuned">Stay tuned!</h2>
<p>title: “Week 10 (July 20 - July 26)” layout: single classes: wide permalink: /blog/gsoc-2021/report/week-10/ excerpt: "" modified: last_modified_at: 2021-07-12 —</p>
<p>We decide to choose “<strong>from-to</strong>” with a proxmity window of 4 word tokens between “<strong>from</strong>” and “<strong>to</strong>” as the initial template of lexical trigger to map it to the construal dimension, Prominence. In addition, we also identify “<strong>first-second</strong>”, “<strong>firstly-secondly</strong>” and “<strong>here-then</strong>”, but could not find much relevant hand gestures in the PATS dataset.</p>
<p>Consider a sample video of a talk show host, Jimmy Fallon, taken from the PATS dataset with a pre-defined start and end time:</p>
<p>{% include video id="JY-Nhs__4xk?start=145&amp;end=160" provider=“youtube” %}</p>
<p>Transcript: “with Mexico that players can either travel <strong>from</strong> the u.s. <strong>to</strong> Mexico by plane or just walked past the wall that still won’t be built it’s up to you you can choose”</p>
<p>The video frames corresponding to the “<strong>from</strong>-<strong>to</strong>” lexical trigger are shown below:</p>
<!-- Left Column -->
<div style="width: 50%; min-width: 300px; float: left;">
<!-- Video 1 -->
<div style="background-color: whitesmoke; padding: 15px; margin: 0 2% 4% 0;">
<pre><code>&lt;h3 style=&quot;border-bottom: 1px solid; margin: 0 0 8px 0;&quot;&gt;Frame 1&lt;/h3&gt;
&lt;div style=&quot;position: relative; width: 100%; padding-top: 56.25%;&quot;&gt;
&lt;iframe src=&quot;https://streamable.com/e/3qs7r5?loop=0&quot; frameborder=&quot;0&quot; width=&quot;100%&quot; height=&quot;100%&quot; allowfullscreen style=&quot;width:100%;height:100%;position:absolute;left:0px;top:0px;overflow:hidden;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p style=&quot;margin: 10px 0 0 0;&quot;&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th&gt;Handedness&lt;/th&gt;
    &lt;th&gt;Axis&lt;/th&gt;
    &lt;th&gt;Shape&lt;/th&gt;
    &lt;th&gt;Direction&lt;/th&gt;
    &lt;th&gt;Gesture&lt;/th&gt;
  &lt;/tr&gt;</code></pre>
</thead>
<tbody>
<tr>
<td>
Both Hands
</td>
<td>
Horizontal
</td>
<td>
Straight
</td>
<td>
Diagonal right up
</td>
<td>
Yes
</td>
</tr>
</tbody>
<caption>
<b>Lexical prompt: </b>“travel from the”
</caption>
</table>
</div>
<!-- Video 2 -->
<div style="background-color: whitesmoke; padding: 15px; margin: 0 2% 4% 0;">
<pre><code>&lt;h3 style=&quot;border-bottom: 1px solid; margin: 0 0 8px 0;&quot;&gt;Frame 2&lt;/h3&gt;
&lt;div style=&quot;position: relative; width: 100%; padding-top: 56.25%;&quot;&gt;
&lt;iframe src=&quot;https://streamable.com/e/1hy1sl?loop=0&quot; frameborder=&quot;0&quot; width=&quot;100%&quot; height=&quot;100%&quot; allowfullscreen style=&quot;width:100%;height:100%;position:absolute;left:0px;top:0px;overflow:hidden;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p style=&quot;margin: 10px 0 0 0;&quot;&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th&gt;Handedness&lt;/th&gt;
    &lt;th&gt;Axis&lt;/th&gt;
    &lt;th&gt;Shape&lt;/th&gt;
    &lt;th&gt;Direction&lt;/th&gt;
    &lt;th&gt;Gesture&lt;/th&gt;
  &lt;/tr&gt;</code></pre>
</thead>
<tbody>
<tr>
<td>
Both Hands
</td>
<td>
Horizontal
</td>
<td>
Straight
</td>
<td>
Leftward
</td>
<td>
Yes
</td>
</tr>
</tbody>
<caption>
<b>Lexical prompt: </b>“u.s. to”
</caption>
</table>
</div>
</div>
<!-- Right Column -->
<div style="width: 50%; min-width: 300px; float: left;">
<!-- Video 3 -->
<div style="background-color: whitesmoke; padding: 15px; margin: 0 0 4% 2%;">
<pre><code>&lt;h3 style=&quot;border-bottom: 1px solid; margin: 0 0 8px 0;&quot;&gt;Frame 3&lt;/h3&gt;
&lt;div style=&quot;position: relative; width: 100%; padding-top: 56.25%;&quot;&gt;
&lt;iframe src=&quot;https://streamable.com/e/ako3bg?loop=0&quot; frameborder=&quot;0&quot; width=&quot;100%&quot; height=&quot;100%&quot; allowfullscreen style=&quot;width:100%;height:100%;position:absolute;left:0px;top:0px;overflow:hidden;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p style=&quot;margin: 10px 0 0 0;&quot;&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
  &lt;tr&gt;
    &lt;th&gt;Handedness&lt;/th&gt;
    &lt;th&gt;Axis&lt;/th&gt;
    &lt;th&gt;Shape&lt;/th&gt;
    &lt;th&gt;Direction&lt;/th&gt;
    &lt;th&gt;Gesture&lt;/th&gt;
  &lt;/tr&gt;</code></pre>
</thead>
<tbody>
<tr>
<td>
Both Hands
</td>
<td>
Horizontal
</td>
<td>
Straight
</td>
<td>
Diagonal left down
</td>
<td>
Yes
</td>
</tr>
</tbody>
<caption>
<b>Lexical prompt: </b>“Mexico by”
</caption>
</table>
</div>
<!-- Video 4 -->
<div style="background-color: whitesmoke; padding: 15px; margin: 0 0 4% 2%;">
<pre><code>&lt;h3 style=&quot;border-bottom: 1px solid; margin: 0 0 8px 0;&quot;&gt;Frame 4&lt;/h3&gt;
&lt;div style=&quot;position: relative; width: 100%; padding-top: 56.25%;&quot;&gt;
  &lt;iframe src=&quot;https://streamable.com/e/u1c6wn?loop=0&quot; frameborder=&quot;0&quot; width=&quot;100%&quot; height=&quot;100%&quot; allowfullscreen style=&quot;width:100%;height:100%;position:absolute;left:0px;top:0px;overflow:hidden;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p style=&quot;margin: 10px 0 0 0;&quot;&gt;&lt;/p&gt;</code></pre>
<table>
<thead>
<tr>
<th>
Handedness
</th>
<th>
Axis
</th>
<th>
Shape
</th>
<th>
Direction
</th>
<th>
Gesture
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Both Hands
</td>
<td>
Horizontal
</td>
<td>
Straight
</td>
<td>
Rightward
</td>
<td>
Yes
</td>
</tr>
</tbody>
<caption>
<b>Lexical prompt: </b>“plane”
</caption>
</table>
</div>
</div>
<p>The problem statement can be framed as:</p>
<blockquote>
<p>Given a video input, identify whether a hand gesture is present corresponding to the hand gestures portrayed by the speaker during the enunciation of the “from-to” lexical trigger in the training video frames. If it is, then classify the video frame with the different gesture types (handedness, axis, shape, direction). If it is not, then classify the video frame as “No Gesture”.</p>
</blockquote>
<p>We look to segment a video into video frames of equal time-duration of 500 ms. Furthermore, to create a set of True Positive (TP) instances, we extract the video frames corresponding to the start and ending portions of the lexical trigger. To create a set of True Negative (TN) instances, we extract the video frames and annotate the ones having hand gestures unrelated to the ones found in the TP set. We perform the annotations using the Red Hen Rapid Annotator tool.</p>
<p>The classification model comprises of mainly three units: positional embedding to enable the model access to the pixel order information, Transformer encoder to process the source sequence, and a max-pooling layer to keep the most important feature.</p>
<p>The Model architecture is shown below:</p>
<figure>
<img src="https://i.imgur.com/jazJ48X.png" alt="Model Architecture" /><figcaption>Model Architecture</figcaption>
</figure>
<p>Since a video frame can be accompanied by multiple hand gesture types, it makes sense to treat it as a multi-label classification problem as the labels are not mutually exclusive.</p>
<p>I spend the last week of the GSoC period on packaging the final product into a singularity container (which is a requirement set by Red Hen). To build a container, we first need a definition file:</p>
<p>Bootstrap: docker From: tensorflow/tensorflow:latest-gpu</p>
<p>%labels AUTHOR nickilmaveli@gmail.com</p>
<p>%post apt-get update &amp;&amp; apt-get -y install git ffmpeg libsm6 libxext6 -y cd / &amp;&amp; git clone https://github.com/Nickil21/joint-meaning-construal.git pip3 install pandas opencv-python numpy tables joblib imageio openpyxl flask jinja2 git+https://github.com/tensorflow/docs</p>
<p>%runscript cd /joint-meaning-construal/ &amp;&amp; python3 detect_gesture.py</p>
<p>We can then build the image using the <a href="https://cloud.sylabs.io/builder">Sylabs Cloud Builder</a> by uploading the definition file. The build takes about 15-20 mins to complete. Once the image is built, the steps to run inside the singularity container are as follows:</p>
<ol type="1">
<li>Log on to the <a href="https://ondemand.case.edu/">CWRU HPC OnDemand Web Portal</a>.</li>
<li>Click on Clusters and choose “rider Shell Access” from the drop-down menu. This should redirect you to a Terminal console window of the HPC server.</li>
<li><p>Enter the gallina home directory:</p>
<pre><code>[nxm526@hpc4 ~]$ cd /mnt/rds/redhen/gallina/home/nxm526/</code></pre></li>
<li><p>Load the Singularity module:</p>
<pre><code>[nxm526@hpc4 nxm526]$ module load singularity</code></pre></li>
<li><p>Pull with Singularity:</p>
<pre><code>[nxm526@hpc4 nxm526]$ singularity pull library://nickil21/default/image:latest</code></pre></li>
<li><p>Move the Singularity image inside the project folder:</p>
<pre><code>[nxm526@hpc4 nxm526]$ mv image_latest.sif joint-meaning-construal/singularity/</code></pre></li>
<li><p>Enter the project root folder:</p>
<pre><code>[nxm526@hpc4 nxm526]$ cd joint-meaning-construal/</code></pre></li>
<li><p>Execute the command within a container:</p>
<pre><code>[nxm526@hpc4 nxm526]$ singularity exec ./singularity/image_latest.sif python detect_gesture.py &lt;path_to_video_file&gt;</code></pre></li>
<li><h2 id="the-output-files-would-be-present-inside-the-staticuploads-folder.-retrieve-the-output-files-using-the-cwru-hpc-ondemand-web-portal.">The output files would be present inside the <code>static/uploads/</code> folder. Retrieve the output files using the <a href="https://ondemand.case.edu/pun/sys/dashboard/files/fs//mnt/rds/redhen/gallina/home/nxm526/joint-meaning-construal/static/uploads">CWRU HPC OnDemand Web Portal</a>.</h2>
<p>layout: single classes: wide title: “Week 1 (May 18 - May 24)” permalink: /blog/gsoc-2021/report/week-1/ excerpt: "" last_modified_at: 2021-05-30 —</p></li>
</ol>
<p>In this week, Mark creates CWRU (Case Western Reserve University) username accounts for all the students and mentors. Mine is nxm526. To receive all official HPC messages, Mark provides us with a <a href="https://www.case.edu">case.edu</a>{:target="_blank"} account.</p>
<p>To see if we can log in to the CWRU HPC server after connecting through the CWRU VPN:</p>
<p>$ ssh nxm526@rider.case.edu Warning: Permanently added the ECDSA host key for IP address ‘129.22.100.157’ to the list of known hosts. $ nxm526@rider.case.edu’s password:</p>
<p>To access the HOME directory:</p>
<p>$ [nxm526@hpc3 home]$ cd /home/nxm526/ $ [nxm526@hpc3 ~]$ ll total 0</p>
<p>After exchanging a couple of slack messages, Tiago, my primary mentor, schedules a brief discussion on the project with me and the other mentors on May 27th. The meeting takes place via a Zoom call. During the session, we introduce ourselves. The mentors specifically highlight areas in which their strength lies and how I could seek their advice depending on the domain of the problem to leverage their expertise. There is also a separate slack channel <a href="https://app.slack.com/client/TGH1N2VHP/C022UT8UZTR">project_construal_2021</a>{:target="_blank"} to update all the mentors at once at every stage of the project.</p>
<p>Following are some of the key takeaways from the meeting:</p>
<ul>
<li><p>Aim at making small advancements that resembles a minuscule improvement when comparing to the existing systems. Since the topic of detecting meaning from a multimodal input has not been explored extensively due to the difficulty in understanding several interpretations possible, making even a small contribution in the right direction is challenging.</p></li>
<li><p>Even creating an alogrithm for captioning of hand gestures in a 2D co-ordinate space is tricky due to multiple interpretations of the scene understanding by annotators/end-users. As a result, the Inter-Annotator Agreement tends to be quite low in this scenario.</p></li>
</ul>
<h2 id="the-next-day-we-have-our-first-introductory-meeting-with-all-the-12-gsoc-selected-students-under-red-hen-and-the-mentors.-the-meeting-agenda-is-to-introduce-all-the-mentors-mentees-and-get-to-know-the-cohort-better.-each-student-spoke-about-their-project-their-assigned-mentors-and-came-up-with-questions-that-could-benefit-others-in-navigating-the-project-more-smoothly.">The next day, we have our first introductory meeting with all the 12 GSoC selected students under Red Hen and the mentors. The meeting agenda is to introduce all the mentor(s)-mentee(s) and get to know the cohort better. Each student spoke about their project, their assigned mentors and came up with questions that could benefit others in navigating the project more smoothly.</h2>
<p>layout: single classes: wide title: “Week 3 (June 1 - June 7)” permalink: /blog/gsoc-2021/report/week-3/ excerpt: "" modified: last_modified_at: 2021-06-07 — Inorder to comply with the <a href="https://sites.google.com/site/distributedlittleredhen/home/what-kind-of-red-hen-are-you?authuser=0">section 108 of the U.S. Copyright Act</a>{:target="_blank“}, it is necessary that we email <a href="mailto:access@redhenlab.org">access@redhenlab.org</a>{:target=”_blank"} requesting access to the Red Hen data and tools. Finally, I get the access upon submitting the research as well as the contribution proposal.</p>
<p>Due to the space storage constraints in the default HOME directory, it is not advisable to keep files over there. To store files having large sizes, gallina home, which is a directory on gallina (a Red Hen server) needs to be set up.</p>
<p>To check if gallina home is properly set in CWRU HPC:</p>
<p>$ [nxm526@hpc4 home]$ pwd /mnt/rds/redhen/gallina/home $ [nxm526@hpc4 home]$ ls -al nxm526 total 20 drwxrwsr-x 2 nxm526 mbt8 2 Jun 7 18:30 . drwxrwsr-x 91 mbt8 mbt8 91 Jun 7 18:30 .. — layout: single classes: wide title: “Week 4 (June 8 - June 14)” permalink: /blog/gsoc-2021/report/week-4/ excerpt: "" modified: last_modified_at: 2021-06-15 — Since this being the start of the coding period, I did get in touch with my primary mentor, Tiago. We mutually agree that querying the Red Hen dataset that have a particular gesture type can be a good way to investigate construal meaning relationships between the different linguistic elements. To narrow down the numerous possibilities of gesture types, we only consider hand gestures for our ablation.</p>
<p>We choose the following parameters:</p>
<table>
<colgroup>
<col style="width: 64%" />
<col style="width: 35%" />
</colgroup>
<thead>
<tr class="header">
<th>Gesture Type</th>
<th>Values</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Body part</td>
<td>left hand, right hand, both hands</td>
</tr>
<tr class="even">
<td>Axis</td>
<td>vertical, horizontal/lateral</td>
</tr>
<tr class="odd">
<td>Direction</td>
<td>upward, downward, leftward, rightward, diagonal right up, diagonal left up, diagonal right down, diagonal left down</td>
</tr>
<tr class="even">
<td>Shape</td>
<td>straight, arced</td>
</tr>
</tbody>
</table>
<h2 id="to-begin-with-we-want-the-algorithm-to-be-capable-of-understanding-only-the-prominence-dimension-which-is-relevant-in-the-case-of-gestures-and-more-widespread.">To begin with, we want the algorithm to be capable of understanding only the <strong>Prominence</strong> dimension which is relevant in the case of gestures and more widespread.</h2>
<p>title: “Week 5 (June 15 - June 21)” layout: single classes: wide permalink: /blog/gsoc-2021/report/week-5/ excerpt: "" modified: last_modified_at: 2021-06-22 — In this week, I schedule a Zoom meeting on June 16<sup>th</sup> with Tiago to discuss about the next steps. Here’s a gist of the discussion that took place:</p>
<h2 id="objective">Objective</h2>
<p>We want to map the Frames into a particular construal dimension. Even though the Terminal nodes may be somewhat random in meaning, using the FrameNet’s rich network-based parsing mechanisms, we can perhaps leverage the upper levels in the graph to map it to a specific construal dimension.</p>
<h2 id="method">Method</h2>
<ol type="1">
<li>For the Red Hen dataset, we need to retrieve Frames that have the gesture types we want to query for. To obtain these Frames, we can use a timestamp interval of +/- 3 seconds from the annotated gesture types. The Frame is made up of two components - an audio transcription in the form of a textual output and a Frame Metadata.</li>
<li>The FrameNet API can be used to model the Frame Metadata and carry out the Frame-based Semantic parsing. For instance, we can leverage the rich APIs available to derive relationships between all the FrameNet frames in the network. FrameNet has top-level concepts as Event, Relation, State, Entity, Locale, and Process. Most of the individual Frames in the network inherit these top-level concepts and are related to one another using a relationship such as Using, Precedes, Subframe, etc.</li>
<li>Once we have some form of a relationship among Frames, we need to create a representation of these Frames.</li>
<li>After getting the representations, we can cluster using a simple K-Means Clustering algorithm.</li>
<li>We can use these clusters to segment the Frames into Lexical units.</li>
<li>The Lexical units can be mapped to a representation using a contextual word embeddings method.</li>
<li>These representations can be fed as input to the Generator, which outputs a sequence. Finally, we feed this to the Discriminator.</li>
<li>Next, we can feed the textual output (audio transcription) to the Discriminator.</li>
<li>In the last step, Adversarial training happens using GANs to determine if the final outcome is a real or a fake transcription. The Discriminator is trained to classify whether samples are from the Generator or from the real data distribution. The objective of the Generator is to produce samples that are indistinguishable by the Discriminator.</li>
<li>Through the Human-In-Loop intervention, we can create synthetic datasets to evaluate whether the correct construal dimension is associated with a Frame or not.</li>
<li>The idea is to start with maybe two types of construal dimension – Prominence and Configuration and analyze if we are doing better than a random guess. Later, we can move on to cover the other dimensions.</li>
</ol>
<h2 id="resources">Resources</h2>
<ul>
<li><a href="https://framenet.icsi.berkeley.edu/fndrupal/FrameLatticeList">FrameLatticeList</a></li>
<li>Semantic Role Labeling - <a href="http://www.cs.cmu.edu/~ark/SEMAFOR/">SEMAFOR</a>, <a href="https://github.com/google/sling">Sling</a>, <a href="https://github.com/swabhs/open-sesame">Open Sesame</a></li>
<li><a href="https://github.com/FrameNetBrasil/webtool">Webtool Repository</a></li>
<li><a href="https://framenet.icsi.berkeley.edu/fndrupal/node/5561">Multilingual FrameNet</a></li>
<li><a href="https://www.aclweb.org/anthology/D17-2001/">NLTK FrameNet</a></li>
</ul>
<h2 id="timeline">Timeline</h2>
<p>For the first evaluation period, we hope to have the following components ready:</p>
<ul>
<li>Retrieving Frames corresponding to the chosen gesture types from Red Hen datasets.</li>
<li>Establishing relationships among different Frames and creating Frame representations.</li>
<li>Clustering Frames into groups based on these representations.</li>
<li>Running ablation using only the speech transcription and see if any meaningful linguistic phenomena can be observed. Getting some form of a metric to show as an evaluation.</li>
</ul>
<p>For the final evaluation period, we hope to complete:</p>
<ul>
<li>The Lexical units can be mapped to a representation.</li>
<li>Once we have the speech transcription as well as the Lexical representation for each Frame pertaining to the gestures, we can jointly train a model to see if the added gesture component performs better than only using the linguistic component.</li>
<li>Optimize and change the type of Neural Network model depending on the reported performance. Decide whether a GAN-based or a Transformer based architecture would be more suitable during the modeling process.</li>
<li>Extend to other dimensions one at a time beyond Prominence and Configuration.</li>
<li><h2 id="create-a-singluarity-container-and-integrate-with-the-red-hen-codebase.">Create a Singluarity container and integrate with the Red Hen codebase.</h2>
title: “Week 6 (June 22 - June 28)” layout: single classes: wide permalink: /blog/gsoc-2021/report/week-6/ excerpt: "" modified: last_modified_at: 2021-06-29 — As a follow-up to the discussion which took place last week, I start with extracting the text (audio transcriptions) that was spoken within a timestamp, containing a specific value of the chosen gesture type from the ELAN files (.eaf format) corresponding to the Ellen Interviews dataset. These are provided to me by Elizabeth Mahoney containing 30 videos with ELAN annotations and the transcription files. The next objective is, we can choose to run the FrameNet API on top of these text instances to do the semantic parsing.</li>
</ul>
<p>The following script basically summarizes how to interact with the ELAN files to obtain the annotations according to the Tier Type/Name/ID. Finally, we save the gesture types between the start and end times containing a clause which is a transcribed text.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" title="1"><span class="im">import</span> pympi</a>
<a class="sourceLine" id="cb11-2" title="2"><span class="im">import</span> pandas  <span class="im">as</span> pd</a>
<a class="sourceLine" id="cb11-3" title="3"></a>
<a class="sourceLine" id="cb11-4" title="4"><span class="co"># Tier names we want to query for containing the gesture types</span></a>
<a class="sourceLine" id="cb11-5" title="5">tier_names <span class="op">=</span> [<span class="st">&#39;clauses&#39;</span>, <span class="st">&#39;Handshape&#39;</span>, <span class="st">&#39;Movement direction&#39;</span>, <span class="st">&#39;Handedness&#39;</span>, <span class="st">&#39;Axis&#39;</span>]</a>
<a class="sourceLine" id="cb11-6" title="6"></a>
<a class="sourceLine" id="cb11-7" title="7">list_of_dfs <span class="op">=</span> []</a>
<a class="sourceLine" id="cb11-8" title="8"><span class="co"># Initialize the elan file</span></a>
<a class="sourceLine" id="cb11-9" title="9">eafob <span class="op">=</span> pympi.Elan.Eaf(<span class="st">&quot;input/sample.eaf&quot;</span>)</a>
<a class="sourceLine" id="cb11-10" title="10"><span class="co"># Loop over all the defined tiers that contain orthography</span></a>
<a class="sourceLine" id="cb11-11" title="11"><span class="cf">for</span> tier <span class="kw">in</span> tier_names:</a>
<a class="sourceLine" id="cb11-12" title="12">    <span class="co"># If the tier is not present in the elan file spew an error and</span></a>
<a class="sourceLine" id="cb11-13" title="13">    <span class="co"># continue. This is done to avoid possible KeyErrors</span></a>
<a class="sourceLine" id="cb11-14" title="14">    <span class="cf">if</span> tier <span class="kw">not</span> <span class="kw">in</span> eafob.get_tier_names():</a>
<a class="sourceLine" id="cb11-15" title="15">        <span class="bu">print</span>(<span class="st">&#39;WARNING!!!&#39;</span>)</a>
<a class="sourceLine" id="cb11-16" title="16">        <span class="bu">print</span>(<span class="st">&#39;One of the ortography tiers is not present in the elan file&#39;</span>)</a>
<a class="sourceLine" id="cb11-17" title="17">        <span class="bu">print</span>(<span class="st">&#39;namely: </span><span class="sc">{}</span><span class="st">. skipping this one...&#39;</span>.<span class="bu">format</span>(tier))</a>
<a class="sourceLine" id="cb11-18" title="18">    <span class="co"># If the tier is present we can loop through the annotation data</span></a>
<a class="sourceLine" id="cb11-19" title="19">    <span class="cf">else</span>:</a>
<a class="sourceLine" id="cb11-20" title="20">        lst <span class="op">=</span> []</a>
<a class="sourceLine" id="cb11-21" title="21">        <span class="cf">for</span> annotation <span class="kw">in</span> eafob.get_annotation_data_for_tier(tier):</a>
<a class="sourceLine" id="cb11-22" title="22">            d <span class="op">=</span> {}</a>
<a class="sourceLine" id="cb11-23" title="23">            d[<span class="st">&#39;start_time&#39;</span>] <span class="op">=</span> annotation[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb11-24" title="24">            d[<span class="st">&#39;end_time&#39;</span>] <span class="op">=</span> annotation[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb11-25" title="25">            d[tier] <span class="op">=</span> annotation[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb11-26" title="26">            d[<span class="st">&#39;gesture_phases&#39;</span>] <span class="op">=</span> annotation[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb11-27" title="27">            lst.append(d)</a>
<a class="sourceLine" id="cb11-28" title="28">        df <span class="op">=</span> pd.DataFrame(lst)</a>
<a class="sourceLine" id="cb11-29" title="29">        list_of_dfs.append(df)</a>
<a class="sourceLine" id="cb11-30" title="30"></a>
<a class="sourceLine" id="cb11-31" title="31">data <span class="op">=</span> pd.concat([d.set_index([<span class="st">&#39;start_time&#39;</span>, <span class="st">&#39;end_time&#39;</span>, <span class="st">&#39;gesture_phases&#39;</span>]) <span class="cf">for</span> d <span class="kw">in</span> list_of_dfs],</a>
<a class="sourceLine" id="cb11-32" title="32">                  axis<span class="op">=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb11-33" title="33">data.reset_index(inplace<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb11-34" title="34">data[<span class="st">&#39;axis&#39;</span>] <span class="op">=</span> <span class="va">None</span></a>
<a class="sourceLine" id="cb11-35" title="35">data.sort_values([<span class="st">&#39;start_time&#39;</span>, <span class="st">&#39;end_time&#39;</span>], inplace<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb11-36" title="36">data.drop_duplicates(inplace<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb11-37" title="37">data.to_csv(<span class="st">&quot;output.tsv&quot;</span>, index<span class="op">=</span><span class="va">False</span>, sep<span class="op">=</span><span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>)</a></code></pre></div>
<p>One thing to note is that there could be a possibility of a mismatch between a Frame and its transcription due to the tagging being performed at a granular level of timestamp and a single annotator(presumably) doing all the tagging. Anyway, here’s how the top 10 rows of <code>output.tsv</code> looks like:</p>
<table style="width:100%;">
<colgroup>
<col style="width: 1%" />
<col style="width: 6%" />
<col style="width: 4%" />
<col style="width: 8%" />
<col style="width: 34%" />
<col style="width: 15%" />
<col style="width: 19%" />
<col style="width: 6%" />
<col style="width: 2%" />
</colgroup>
<thead>
<tr class="header">
<th>#</th>
<th>start_time</th>
<th>end_time</th>
<th>gesture_phases</th>
<th>clauses</th>
<th>Handshape</th>
<th>Movement direction</th>
<th>Handedness</th>
<th>axis</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>10872.0</td>
<td>11259.0</td>
<td>str</td>
<td>I had–</td>
<td>flat</td>
<td>LAB</td>
<td>Left</td>
<td></td>
</tr>
<tr class="even">
<td>2</td>
<td>13387.0</td>
<td>13733.0</td>
<td>str</td>
<td>She had been to Disneyland here.</td>
<td>1-2 stretched</td>
<td>down</td>
<td>Left</td>
<td></td>
</tr>
<tr class="odd">
<td>3</td>
<td>14259.0</td>
<td>14716.0</td>
<td>str</td>
<td>And I had an appearance</td>
<td>1-2 stretched, 3-5 bent</td>
<td>PT</td>
<td>Left</td>
<td></td>
</tr>
<tr class="even">
<td>4</td>
<td>15254.0</td>
<td>15515.0</td>
<td>str</td>
<td>same as before (appearance)</td>
<td>1-2 stretched, 3-5 bent</td>
<td>left</td>
<td>Left</td>
<td></td>
</tr>
<tr class="odd">
<td>5</td>
<td>18061.0</td>
<td>18167.0</td>
<td>str</td>
<td>which was weird</td>
<td>1-4 touching, 5 stretched</td>
<td>down</td>
<td>Left</td>
<td></td>
</tr>
<tr class="even">
<td>6</td>
<td>18309.0</td>
<td>18494.0</td>
<td>str</td>
<td>going down to</td>
<td>1-4 touching, 5 stretched</td>
<td>down</td>
<td>Left</td>
<td></td>
</tr>
<tr class="odd">
<td>7</td>
<td>25329.0</td>
<td>25797.0</td>
<td>str</td>
<td>this was very pricess, Tinkle Bell, Snow White</td>
<td>flat</td>
<td>up</td>
<td>Both</td>
<td></td>
</tr>
<tr class="even">
<td>8</td>
<td>26211.0</td>
<td>26979.0</td>
<td>str</td>
<td>SA(Snow White)</td>
<td>flat</td>
<td>up</td>
<td>Both</td>
<td></td>
</tr>
<tr class="odd">
<td>9</td>
<td>27854.0</td>
<td></td>
<td>str</td>
<td>We did the whole thing</td>
<td>1-2 connected</td>
<td>down</td>
<td>Left</td>
<td></td>
</tr>
<tr class="even">
<td>10</td>
<td>28854.0</td>
<td>29892.0</td>
<td>str</td>
<td>the r–, the lunch in the princess castle (illustration)</td>
<td>flat</td>
<td>LAB</td>
<td>Left</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="the-only-issue-we-face-here-is-that-there-is-only-a-handful-amount-of-elan-annotations-available-inside-the-red-hen-repository.-the-ones-with-the-relevant-hand-gesture-tagging-is-even-less-so-we-have-no-option-but-to-look-for-alternative-sources-that-could-meet-our-purposes.">The only issue we face here is that there is only a handful amount of ELAN annotations available inside the Red Hen repository. The ones with the relevant hand gesture tagging is even less, so we have no option but to look for alternative sources that could meet our purposes.</h2>
<p>title: “Week 7 (June 29 - July 5)” layout: single classes: wide permalink: /blog/gsoc-2021/report/week-7/ excerpt: "" modified: last_modified_at: 2021-07-06 —</p>
<p>As a result of not having relevant hand gestures dataset to start the experimentation phase, I reach out to the mailing list of <a href="http://gesturestudies.com/">International Society of Gesture Studies</a> and do manage to get a few responses.</p>
<p>So far, I collect the following datasets:</p>
<ul>
<li>ELAN tagged co-speech hand gestures dataset without the supporting videos shared at <a href="https://osf.io/6y4k8/">Open Science Framework</a> by Fey Parrill.</li>
<li>Co-speech gestures that co-occur with number-related linguistic expressions (“add”, “subtract”) shared by Daniel Alcaraz Carrion.</li>
<li>Hand gestures dataset although related to sign-language motion capture corpora.</li>
</ul>
<h2 id="tiago-and-i-have-a-chat-on-zoom.-in-the-meeting-we-discuss-about-utilizing-the-already-existing-annotation-tool-rapid-hen-annotator-to-quickly-manually-tag-the-pre-defined-gesture-parameters-from-segmented-videos-of-ellen-interview-dataset.-we-choose-about-30-video-segments-to-begin-with.-the-idea-is-to-later-ask-the-annotators-a-template-of-boolean-questions-yesno-corresponding-to-a-particular-construal-dimension-for-eg.-does-the-highlighted-text-invoke-ordering-of-items-in-a-sequence-does-the-highlighed-text-depict-a-time-lapse-does-the-highlighed-text-signify-levels-of-importance-and-so-on.">Tiago and I have a chat on Zoom. In the meeting, we discuss about utilizing the already existing annotation tool, Rapid Hen Annotator, to quickly manually tag the pre-defined gesture parameters from segmented videos of Ellen interview dataset. We choose about 30 video segments to begin with. The idea is to later ask the annotators a template of boolean questions (yes/no) corresponding to a particular construal dimension, for eg., <em>“does the highlighted text invoke ordering of items in a sequence?”</em>, <em>“does the highlighed text depict a time lapse?”</em>, <em>“does the highlighed text signify levels of importance?”</em> and so on.</h2>
<p>title: “Week 8 (July 6 - July 12)” layout: single classes: wide permalink: /blog/gsoc-2021/report/week-8/ excerpt: "" modified: last_modified_at: 2021-07-12 —</p>
