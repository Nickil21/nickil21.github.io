---
title: "Week 7 (June 29 - July 5)"
layout: single
classes: wide
permalink: /blog/gsoc-2021/report/week-07/
excerpt: ""
modified:
last_modified_at: 2021-07-06
---

As a result of not having relevant hand gestures dataset to start the experimentation phase, I reach out to the mailing list of [International Society of Gesture Studies](http://gesturestudies.com/) and do manage to get a few responses.

So far, I collect the following datasets:

* ELAN tagged co-speech hand gestures dataset without the supporting videos shared at [Open Science Framework](https://osf.io/6y4k8/) by Fey Parrill.
* Co-speech gestures that co-occur with number-related linguistic expressions ("add", "subtract") shared by Daniel Alcaraz Carrion.
* Hand gestures dataset although related to sign-language motion capture corpora.

Tiago and I have a chat on Zoom. In the meeting, we discuss about utilizing the already existing annotation tool, Rapid Hen Annotator, to quickly manually tag the pre-defined gesture parameters from segmented videos of Ellen interview dataset. We choose about 30 video segments to begin with. The idea is to later ask the annotators a template of boolean questions (yes/no) corresponding to a particular construal dimension, for eg., *"does the highlighted text invoke ordering of items in a sequence?"*, *"does the highlighed text depict a time lapse?"*, *"does the highlighed text signify levels of importance?"* and so on.